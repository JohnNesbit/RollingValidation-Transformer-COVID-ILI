{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T14:52:06.061980Z",
     "iopub.status.busy": "2024-02-08T14:52:06.061617Z",
     "iopub.status.idle": "2024-02-08T14:52:19.802001Z",
     "shell.execute_reply": "2024-02-08T14:52:19.800421Z",
     "shell.execute_reply.started": "2024-02-08T14:52:06.061951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T14:52:19.807084Z",
     "iopub.status.busy": "2024-02-08T14:52:19.804559Z",
     "iopub.status.idle": "2024-02-08T14:52:32.640229Z",
     "shell.execute_reply": "2024-02-08T14:52:32.639084Z",
     "shell.execute_reply.started": "2024-02-08T14:52:19.807029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simple_colors\n",
      "  Downloading simple_colors-0.1.5-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: simple_colors\n",
      "Successfully installed simple_colors-0.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install simple_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T14:52:32.642221Z",
     "iopub.status.busy": "2024-02-08T14:52:32.641906Z",
     "iopub.status.idle": "2024-02-08T14:52:34.748976Z",
     "shell.execute_reply": "2024-02-08T14:52:34.747978Z",
     "shell.execute_reply.started": "2024-02-08T14:52:32.642192Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "import torch.utils.data as data_utils\n",
    "from functools import partial, reduce\n",
    "from simple_colors import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from metrics import *\n",
    "from DataLoading import *\n",
    "from utils.masking import *\n",
    "from utils.metrics import *\n",
    "from utils.tools import *\n",
    "from utils.timefeatures import *\n",
    "from utils.getmetrics import *\n",
    "from Metrics import *\n",
    "from RWLoading import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.DLinear import *\n",
    "from Models.Autoformer import *\n",
    "from Models.FEDformer import *\n",
    "from Models.FFNN import *\n",
    "from Models.GRU import *\n",
    "from Models.GRUAtt import *\n",
    "from Models.Informer import *\n",
    "from Models.LSTM import *\n",
    "from Models.from Models.LSTMAtt import *\n",
    "from Models.NLinear import *\n",
    "from Models.PatchTST import *\n",
    "from Models.RandomWalk import *\n",
    "from Models.Seq2SeqGRU import *\n",
    "from Models.Seq2SeqLSTM import *\n",
    "from Models.Transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T14:57:01.063903Z",
     "iopub.status.busy": "2024-02-08T14:57:01.063054Z",
     "iopub.status.idle": "2024-02-08T14:57:01.070063Z",
     "shell.execute_reply": "2024-02-08T14:57:01.068894Z",
     "shell.execute_reply.started": "2024-02-08T14:57:01.063867Z"
    }
   },
   "outputs": [],
   "source": [
    "horizons = 4\n",
    "test_ratio = 0.5\n",
    "file_name = '/kaggle/input/pandemic/covid_weekly.csv'\n",
    "features = ['date','new_deaths','icu_patients','hosp_patients','reproduction_rate','new_cases','new_tests','new_vaccinations']\n",
    "epochs = 20\n",
    "patience = 3\n",
    "RNN_layers = 1\n",
    "in_dim = len(features)-1\n",
    "tf = False\n",
    "random_seed = 2021\n",
    "tf_ratio = 0.3\n",
    "models = [\"LSTM\"]\n",
    "RWhorizon = 4\n",
    "batch_size =32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T15:03:08.525526Z",
     "iopub.status.busy": "2024-02-08T15:03:08.525176Z",
     "iopub.status.idle": "2024-02-08T15:03:26.608850Z",
     "shell.execute_reply": "2024-02-08T15:03:26.607954Z",
     "shell.execute_reply.started": "2024-02-08T15:03:08.525498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mlearning_rate: \u001b[0m 0.001 \u001b[1;32m lags: \u001b[0m 8 \u001b[1;32m batch_size: \u001b[0m 4 \u001b[1;32m IFmodel_dim: \u001b[0m 256 \u001b[1;32m layers: \u001b[0m 2 1\n",
      "\u001b[1;34mLSTM\u001b[0m\n",
      "\u001b[1;32mlearning_rate: \u001b[0m 0.001 \u001b[1;32m lags: \u001b[0m 4 \u001b[1;32m batch_size: \u001b[0m 4 \u001b[1;32m IFmodel_dim: \u001b[0m 128\n",
      "[14.790402183049842, 17.05072378810448, 18.791411194620252, 23.74427167675163] 18.59420221063155\n",
      "[750.8900316455696, 868.1534810126582, 984.9107990506329, 1236.178303006329] 960.0331536787974\n",
      "[0.12805492063111898, 0.14805277088020422, 0.16796425928043413, 0.2108148260961605] 0.16372169422197946\n"
     ]
    }
   ],
   "source": [
    "for learning_rate in [0.001]:\n",
    "        for lags in [8]:\n",
    "            for layers in [2]:\n",
    "                for RNN_hidden_size in [256]:\n",
    "                    for m in models:\n",
    "                        print(green('learning_rate: ','bold'),learning_rate, green(' lags: ','bold'),lags,green(' batch_size: ','bold')\n",
    "                              ,batch_size, green(' IFmodel_dim: ','bold'),RNN_hidden_size,green(' layers: ','bold'),layers,layers-1)\n",
    "                        forecast_matrix = pd.DataFrame(columns = ['date'])\n",
    "                        print(blue(m,'bold'))\n",
    "                        random.seed(random_seed)\n",
    "                        np.random.seed(random_seed)\n",
    "                        torch.manual_seed(random_seed)\n",
    "                        data, observed = data_processing(file_name, features)\n",
    "                        scalers, df = data_transform_std(data, test_ratio)\n",
    "                        obs_scalers, observed_df = data_transform_std(observed, 1.0)\n",
    "                        x, y = make_input_output_sequences(data.values, lags, horizons, True)\n",
    "                        x_train, x_test, y_train, y_test = train_test_split(x, y, test_ratio)\n",
    "                        if(m == 'FFNN'):\n",
    "                            learning_rate = 0.001\n",
    "                            batch_size = 32\n",
    "                            lags = 4\n",
    "                            RNN_hidden_size = 512\n",
    "                            print(green('learning_rate: ','bold'),learning_rate, green(' lags: ','bold'),lags,green(' batch_size: ','bold')\n",
    "                              ,batch_size, green(' IFmodel_dim: ','bold'),RNN_hidden_size)\n",
    "                            model = FFNN(RNN_hidden_size,RNN_hidden_size, lags, horizons, len(features)-1).to(device)\n",
    "                        elif(m == 'LSTM'):\n",
    "                            learning_rate = 0.001\n",
    "                            batch_size = 4\n",
    "                            RNN_hidden_size = 128\n",
    "                            lags = 4\n",
    "                            print(green('learning_rate: ','bold'),learning_rate, green(' lags: ','bold'),lags,green(' batch_size: ','bold')\n",
    "                              ,batch_size, green(' IFmodel_dim: ','bold'),RNN_hidden_size)\n",
    "                            model = LSTM(RNN_hidden_size, RNN_layers, lags, horizons, len(features)-1).to(device)\n",
    "                        elif(m == 'GRU'):\n",
    "                            learning_rate = 0.001\n",
    "                            batch_size = 16\n",
    "                            RNN_hidden_size = 128\n",
    "                            lags = 4\n",
    "                            print(green('learning_rate: ','bold'),learning_rate, green(' lags: ','bold'),lags,green(' batch_size: ','bold')\n",
    "                              ,batch_size, green(' IFmodel_dim: ','bold'),RNN_hidden_size)\n",
    "                            model = GRU(RNN_hidden_size, RNN_layers, lags, horizons, len(features)-1).to(device)\n",
    "                        elif(m == 'LSTM_Seq2Seq'):\n",
    "                            learning_rate = 0.001\n",
    "                            batch_size = 32\n",
    "                            RNN_hidden_size = 128\n",
    "                            lags = 4\n",
    "                            print(green('learning_rate: ','bold'),learning_rate, green(' lags: ','bold'),lags,green(' batch_size: ','bold')\n",
    "                              ,batch_size, green(' IFmodel_dim: ','bold'),RNN_hidden_size)\n",
    "                            model = LSTM_Seq2Seq(RNN_hidden_size, RNN_layers, lags, horizons, len(features)-1).to(device)\n",
    "                        elif(m == 'GRU_Seq2Seq'):\n",
    "                            learning_rate = 0.001\n",
    "                            batch_size = 4\n",
    "                            RNN_hidden_size = 64\n",
    "                            lags = 4\n",
    "                            print(green('learning_rate: ','bold'),learning_rate, green(' lags: ','bold'),lags,green(' batch_size: ','bold')\n",
    "                              ,batch_size, green(' IFmodel_dim: ','bold'),RNN_hidden_size)\n",
    "                            model = GRU_Seq2Seq(RNN_hidden_size, RNN_layers, lags, horizons, len(features)-1).to(device)\n",
    "                        elif(m == 'LSTM_Seq2Seq_Att'):\n",
    "                            learning_rate = 0.001\n",
    "                            batch_size = 8\n",
    "                            RNN_hidden_size = 64\n",
    "                            lags = 4\n",
    "                            print(green('learning_rate: ','bold'),learning_rate, green(' lags: ','bold'),lags,green(' batch_size: ','bold')\n",
    "                              ,batch_size, green(' IFmodel_dim: ','bold'),RNN_hidden_size)\n",
    "                            model = LSTM_Seq2Seq_Att(RNN_hidden_size, RNN_layers, lags, horizons, len(features)-1).to(device)\n",
    "                        elif(m == 'GRU_Seq2Seq_Att'):\n",
    "                            learning_rate = 0.001\n",
    "                            batch_size = 16\n",
    "                            RNN_hidden_size = 512\n",
    "                            lags = 6\n",
    "                            print(green('learning_rate: ','bold'),learning_rate, green(' lags: ','bold'),lags,green(' batch_size: ','bold')\n",
    "                              ,batch_size, green(' IFmodel_dim: ','bold'),RNN_hidden_size)\n",
    "                            model = GRU_Seq2Seq_Att(RNN_hidden_size, RNN_layers, lags, horizons, len(features)-1).to(device)\n",
    "                        elif (m == \"Nlinear\"):\n",
    "                            class Configs(object):\n",
    "                                ab = 0\n",
    "                                modes = 64\n",
    "                                mode_select = 'random'\n",
    "                                version = 'Fourier'\n",
    "                                #version = 'Wavelets'\n",
    "                                moving_avg = [12, 24]\n",
    "                                L = 3\n",
    "                                base = 'legendre'\n",
    "                                cross_activation = 'tanh'\n",
    "                                seq_len = lags\n",
    "                                label_len = horizons\n",
    "                                pred_len = horizons\n",
    "                                output_attention = True\n",
    "                                enc_in = in_dim\n",
    "                                individual = True\n",
    "                                dec_in = in_dim\n",
    "                                d_model = RNN_hidden_size\n",
    "                                embed = 'timeF'\n",
    "                                dropout = 0.05\n",
    "                                freq = 'd'\n",
    "                                factor = 1\n",
    "                                n_heads = 8\n",
    "                                d_ff = 4\n",
    "                                e_layers = 1\n",
    "                                d_layers = 1\n",
    "                                distil=False\n",
    "                                c_out = in_dim\n",
    "                                activation = 'gelu'\n",
    "                                wavelet = 0\n",
    "\n",
    "                            config = Configs()\n",
    "                            model = Nlinear(config).to(device)\n",
    "                        elif (m == \"PatchTST\"):\n",
    "                            class Configs(object):\n",
    "                                task_name = 'long_term_forecast'\n",
    "                                ab = 0\n",
    "                                modes = 64\n",
    "                                mode_select = 'random'\n",
    "                                version = 'Fourier'\n",
    "                                #version = 'Wavelets'\n",
    "                                moving_avg = [12, 24]\n",
    "                                L = 3\n",
    "                                base = 'legendre'\n",
    "                                cross_activation = 'tanh'\n",
    "                                seq_len = lags\n",
    "                                label_len = horizons\n",
    "                                pred_len = horizons\n",
    "                                output_attention = True\n",
    "                                enc_in = in_dim\n",
    "                                dec_in = in_dim\n",
    "                                d_model = RNN_hidden_size\n",
    "                                embed = 'timeF'\n",
    "                                dropout = 0.05\n",
    "                                freq = 'h'\n",
    "                                factor = 1\n",
    "                                n_heads = 8\n",
    "                                d_ff = RNN_hidden_size\n",
    "                                e_layers = 4\n",
    "                                d_layers = 4\n",
    "                                distil=False\n",
    "                                c_out = in_dim\n",
    "                                activation = 'gelu'\n",
    "                                wavelet = 0\n",
    "                                \n",
    "\n",
    "                            config = Configs()\n",
    "                            model = PatchTST(config).to(device)\n",
    "                        elif (m == \"Transformer\"):\n",
    "                            class Configs(object):\n",
    "                                ab = 0\n",
    "                                modes = 64\n",
    "    #                             mode_select = 'random'\n",
    "                                version = 'Fourier'\n",
    "                                #version = 'Wavelets'\n",
    "                                moving_avg = [12,24]\n",
    "                                base = 'legendre'\n",
    "                                cross_activation = 'tanh'\n",
    "                                seq_len = lags\n",
    "                                label_len = horizons\n",
    "                                pred_len = horizons\n",
    "                                output_attention = False\n",
    "                                enc_in = in_dim\n",
    "                                dec_in = in_dim\n",
    "                                d_model = RNN_hidden_size\n",
    "                                embed = 'timeF'\n",
    "                                dropout = 0.05\n",
    "                                freq = 'h'\n",
    "                                factor = 1\n",
    "                                n_heads = 8\n",
    "                                d_ff = 2048\n",
    "                                e_layers = layers\n",
    "                                d_layers = layers-1\n",
    "                                distil=True\n",
    "                                c_out = in_dim\n",
    "                                activation = 'gelu'\n",
    "                                wavelet = 0\n",
    "\n",
    "\n",
    "                            config = Configs()\n",
    "                            model = vTransformer(config).to(device)\n",
    "                        elif (m == \"Informer\"):\n",
    "                            class Configs(object):\n",
    "                                ab = 0\n",
    "                                modes = 64\n",
    "    #                             mode_select = 'random'\n",
    "                                version = 'Fourier'\n",
    "                                #version = 'Wavelets'\n",
    "                                moving_avg = [12,24]\n",
    "                                base = 'legendre'\n",
    "                                cross_activation = 'tanh'\n",
    "                                seq_len = lags\n",
    "                                label_len = horizons\n",
    "                                pred_len = horizons\n",
    "                                output_attention = False\n",
    "                                enc_in = in_dim\n",
    "                                dec_in = in_dim\n",
    "                                d_model = RNN_hidden_size\n",
    "                                embed = 'timeF'\n",
    "                                dropout = 0.05\n",
    "                                freq = 'h'\n",
    "                                factor = 1\n",
    "                                n_heads = 8\n",
    "                                d_ff = 2048\n",
    "                                e_layers = layers\n",
    "                                d_layers = layers-1\n",
    "                                distil=True\n",
    "                                c_out = in_dim\n",
    "                                activation = 'gelu'\n",
    "                                wavelet = 0\n",
    "\n",
    "\n",
    "                            config = Configs()\n",
    "                            model = Informer(config).to(device)\n",
    "                        elif (m == \"Autoformer\"):\n",
    "                            class Configs(object):\n",
    "                                ab = 0\n",
    "                                modes = 64\n",
    "                                mode_select = 'random'\n",
    "                                version = 'Fourier'\n",
    "                                #version = 'Wavelets'\n",
    "                                moving_avg = [25, 25]\n",
    "                                L = 3\n",
    "                                base = 'legendre'\n",
    "                                cross_activation = 'tanh'\n",
    "                                seq_len = lags\n",
    "                                label_len = horizons\n",
    "                                pred_len = horizons\n",
    "                                output_attention = True\n",
    "                                enc_in = in_dim\n",
    "                                dec_in = in_dim\n",
    "                                d_model = RNN_hidden_size\n",
    "                                embed = 'timeF'\n",
    "                                dropout = 0.05\n",
    "                                freq = 'w'\n",
    "                                factor = 1\n",
    "                                n_heads = 8\n",
    "                                d_ff = 2048\n",
    "                                e_layers = layers\n",
    "                                d_layers = layers-1\n",
    "                                distil=False\n",
    "                                c_out = in_dim\n",
    "                                activation = 'gelu'\n",
    "                                wavelet = 0\n",
    "                            config = Configs()\n",
    "                            model = Autoformer(config).to(device)\n",
    "                        elif (m == \"FEDFormer\"):\n",
    "                            class Configs(object):\n",
    "                                ab = 0\n",
    "                                modes = 64\n",
    "                                mode_select = 'random'\n",
    "                                version = 'Fourier'\n",
    "                                #version = 'Wavelets'\n",
    "                                moving_avg = [25, 25]\n",
    "                                L = 3\n",
    "                                base = 'legendre'\n",
    "                                cross_activation = 'tanh'\n",
    "                                seq_len = lags\n",
    "                                label_len = lags - horizons\n",
    "                                pred_len = horizons\n",
    "                                output_attention = True\n",
    "                                enc_in = in_dim\n",
    "                                dec_in = in_dim\n",
    "                                d_model = RNN_hidden_size\n",
    "                                embed = 'timeF'\n",
    "                                dropout = 0.05\n",
    "                                freq = 'd'\n",
    "                                factor = 1\n",
    "                                n_heads = 8\n",
    "                                d_ff = 4\n",
    "                                e_layers = 3\n",
    "                                d_layers = 2\n",
    "                                distil=False\n",
    "                                c_out = in_dim\n",
    "                                activation = 'gelu'\n",
    "                                wavelet = 0\n",
    "\n",
    "                            config = Configs()\n",
    "                            model = FEDFormer(config).to(device)\n",
    "                        smapeListPred = []\n",
    "                        smapeListTrue = []\n",
    "                        maePred=[]\n",
    "                        maeTrue=[]\n",
    "                        for w in range(x_test.shape[0]):\n",
    "                            n_epochs_stop = patience\n",
    "                            epochs_no_improve = 0\n",
    "                            early_stop = False\n",
    "                            min_val_loss = np.Inf\n",
    "                   \n",
    "                            x_train_tensor = torch.from_numpy(np.array(x_train[:,:,1:], dtype=np.float32)).float().to(device)\n",
    "                            y_train_tensor = torch.from_numpy(np.array(y_train[:,:,1:], dtype=np.float32)).float().to(device)\n",
    "                            x_test_tensor = torch.from_numpy(np.array(x_test[:,:,1:], dtype=np.float32)).float().to(device)\n",
    "                            y_test_tensor= torch.from_numpy(np.array(y_test[:,:,1:], dtype=np.float32)).float().to(device)\n",
    "                          \n",
    "                            loss_fn = torch.nn.MSELoss()\n",
    "                            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                            train = data_utils.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "                            train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "                            lr = learning_rate\n",
    "                            for epoch in range(epochs):\n",
    "                                train_losses = []\n",
    "                                model.train()\n",
    "                                for x_t, y_t in train_loader:\n",
    "                                    optimizer.zero_grad()\n",
    "                                    y_pred = model(x_t, y_t, tf)\n",
    "                                    loss = loss_fn(y_pred, y_t[:,:,0])\n",
    "                                    loss.backward()\n",
    "                                    optimizer.step()\n",
    "                                    train_losses.append(loss.item())\n",
    "                                train_loss = np.average(train_losses)\n",
    "                                if train_loss < min_val_loss:\n",
    "                                    epochs_no_improve = 0\n",
    "                                    min_val_loss = train_loss\n",
    "                                else:\n",
    "                                    epochs_no_improve += 1\n",
    "                                if epochs_no_improve == n_epochs_stop:\n",
    "                                    break\n",
    "          \n",
    "                            with torch.no_grad():\n",
    "                                model.eval()\n",
    "                                y_test_pred = model(x_test_tensor, y_test_tensor, False)\n",
    "\n",
    "                                smapeListTrue.append(scalers['scaler_new_deaths'].inverse_transform(y_test_tensor[:, :, 0].cpu().numpy())[0,:])\n",
    "                                smapeListPred.append(scalers['scaler_new_deaths'].inverse_transform(y_test_pred.cpu().numpy())[0,:])\n",
    "                                maeTrue.append(y_test_tensor[:, :, 0].cpu().numpy()[0,:])\n",
    "                                maePred.append(y_test_pred.cpu().numpy()[0,:])\n",
    "                            x_train, x_test, y_train, y_test = shift_sequence(x_train, y_train, x_test, y_test, 1, True)\n",
    "                           \n",
    "                        smapeListTrue = np.array(smapeListTrue)\n",
    "                        smapeListPred = np.array(smapeListPred)\n",
    "                        maeTrue = np.array(maeTrue)\n",
    "                        maePred = np.array(maePred)\n",
    "\n",
    "                        sMAPEmean=[smape(smapeListTrue[:,h], smapeListPred[:,h]) for h in range(4)]\n",
    "                        print(sMAPEmean,np.mean(sMAPEmean))\n",
    "                        MAEmean=[mae(smapeListTrue[:,h], smapeListPred[:,h]) for h in range(4)]\n",
    "                        print(MAEmean,np.mean(MAEmean))\n",
    "                        normMAEmean=[mae(maeTrue[:,h], maePred[:,h]) for h in range(4)]\n",
    "                        print(normMAEmean,np.mean(normMAEmean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 4760582,
     "datasetId": 2497587,
     "sourceId": 4698165,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 7684247,
     "datasetId": 4036814,
     "sourceId": 7589289,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
